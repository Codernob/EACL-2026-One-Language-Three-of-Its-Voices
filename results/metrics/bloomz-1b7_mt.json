{
  "bleu": 2.3509488406173875,
  "chrf": 21.224115295956366,
  "bleu_details": {
    "precisions": [
      19.07270001153358,
      2.7666719974412284,
      1.0327739141298464,
      0.5605283740331972
    ],
    "brevity_penalty": 1.0,
    "length_ratio": 1.2082965578111209
  },
  "sample_bleu_mean": 3.338838118348606,
  "sample_bleu_std": 4.48754764019724,
  "total": 999,
  "model": "bloomz-1b7",
  "task": "mt",
  "num_samples": 999,
  "source_analysis": {
    "flores200": {
      "num_samples": 822,
      "accuracy": null,
      "f1": null,
      "bleu": 2.526050617320152,
      "exact_match": null
    },
    "tatoeba": {
      "num_samples": 177,
      "accuracy": null,
      "f1": null,
      "bleu": 0.1505782772854354,
      "exact_match": null
    }
  },
  "variant_breakdown": {
    "dari": {
      "num_samples": 333,
      "accuracy": null,
      "f1": null,
      "bleu": 3.4195090901585004,
      "chrf": 24.814264593494183,
      "exact_match": null
    },
    "persian": {
      "num_samples": 333,
      "accuracy": null,
      "f1": null,
      "bleu": 2.3547479040606425,
      "chrf": 20.724718151109144,
      "exact_match": null
    },
    "tajik": {
      "num_samples": 333,
      "accuracy": null,
      "f1": null,
      "bleu": 1.0496137763508986,
      "chrf": 17.91037211778441,
      "exact_match": null
    }
  },
  "variant_analysis": {
    "variant_scores": {
      "dari": 3.4195090901585004,
      "persian": 2.3547479040606425,
      "tajik": 1.0496137763508986
    },
    "mean_score": 2.274623590190014,
    "std_score": 0.9691631712829156,
    "min_score": 1.0496137763508986,
    "max_score": 3.4195090901585004,
    "score_range": 2.3698953138076018,
    "metric_used": "bleu",
    "best_variant": "dari",
    "worst_variant": "tajik",
    "performance_gap": 2.3698953138076018
  },
  "inference_time_seconds": 473.1864845752716
}